{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de las bibliotecas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import datetime\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras import Model \n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lectura y preprocesamiento de datos (generador de imágenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Mostrar algunas imágenes de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Fried Noodles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = load_img('data/train/noodles_fried/3685231_a86b9a9ec2_b.jpg')\n",
    "plt.figure(figsize = (4,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Fried Noodles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = img_to_array(img)\n",
    "x2 = x.reshape((1,) + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.title('Pre-processed')\n",
    "for batch in datagen.flow(x2, batch_size = 1):\n",
    "    i += 1\n",
    "    if i > 9:\n",
    "        break\n",
    "    temp = batch.reshape(x.shape)\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.imshow(temp.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Noodle soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = load_img('data/train/noodles_soup/5153183161_eb845fb8d9_b.jpg')\n",
    "plt.figure(figsize = (4,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Noodle Soup')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img_to_array(img)\n",
    "x2 = x.reshape((1,) + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.title('Pre-processed')\n",
    "for batch in datagen.flow(x2, batch_size = 1):\n",
    "    i += 1\n",
    "    if i > 9:\n",
    "        break\n",
    "    temp = batch.reshape(x.shape)\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.imshow(temp.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Lectura de datos de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuración por defecto\n",
    "img_width, img_height = 299, 299\n",
    "\n",
    "train_dir = 'data/train'\n",
    "validate_dir = 'data/validate'\n",
    "batch_size = 32\n",
    "nb_classes = len(glob.glob(train_dir + '/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Preprocesamiento de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-procesamiento de datos para entrenamiento\n",
    "train_datagen =  ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    fill_mode = 'nearest',\n",
    "    horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-procesamiento de datos para su validación\n",
    "validate_datagen =  ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    fill_mode = 'nearest',\n",
    "    horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generar y almacenar datos de entrenamiento\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generar y almacenar datos de validación\n",
    "validate_generator = validate_datagen.flow_from_directory(\n",
    "    validate_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construcción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Configurar la transferencia del conocimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure el aprendizaje de transferencia en el modelo ImageNet VGG19 previamente entrenado: \n",
    "# elimine la capa completamente conectada y reemplace con softmax para clasificar 2 clases\n",
    "vgg19_model = VGG19(weights = 'imagenet', include_top = False)\n",
    "x = vgg19_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(nb_classes, activation = 'softmax')(x)\n",
    "model = Model(vgg19_model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# congelar todas las capas del modelo previamente entrenado\n",
    "for layer in vgg19_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Compilar el nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile el nuevo modelo usando un optimizador RMSProp\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenar el nuevo modelo mostrando el tiempo total de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ajustar el modelo, registrar los resultados y el tiempo de entrenamiento\n",
    "now = datetime.datetime.now\n",
    "t = now()\n",
    "transfer_learning_history = model.fit(\n",
    "    train_generator,\n",
    "    epochs = 20,\n",
    "    steps_per_epoch = len(train_generator),\n",
    "    validation_data = validate_generator,\n",
    "    validation_steps = len(validate_generator), verbose = 1)\n",
    "print('Tiempo de entrenamiento: %s' % (now() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluar el desempeño del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluar el rendimiento del nuevo modelo e informar de los resultados\n",
    "score = model.evaluate_generator(validate_generator, len(validate_generator))\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Guardar el modelo de aprendizaje por transferencia para fines de predicción fuera de línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/noodles_vgg19_model_tl.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gráficas de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xfer_acc = transfer_learning_history.history['accuracy']\n",
    "val_acc = transfer_learning_history.history['val_accuracy']\n",
    "xfer_loss = transfer_learning_history.history['loss']\n",
    "val_loss = transfer_learning_history.history['val_loss']\n",
    "epochs = range(len(xfer_acc))\n",
    "\n",
    "x = np.array(epochs)\n",
    "y = np.array(xfer_acc)\n",
    "x_smooth = np.linspace(x.min(), x.max(), 500)\n",
    "y_smooth = make_interp_spline(x,y)(x_smooth)\n",
    "plt.plot(x_smooth, y_smooth, 'r-', label = 'Entrenamiento')\n",
    "\n",
    "x1 = np.array(epochs)\n",
    "y1 = np.array(val_acc)\n",
    "x1_smooth = np.linspace(x1.min(), x1.max(), 500)\n",
    "y1_smooth = make_interp_spline(x1,y1)(x1_smooth)\n",
    "\n",
    "plt.plot(x1_smooth, y1_smooth, 'g-', label = 'Validación')\n",
    "plt.title('Transfer Learning - Precisión de entrenamiento y validación')\n",
    "plt.legend(loc = 'lower left', fontsize = 9)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "plt.ylim(0,1.05)\n",
    "\n",
    "plt.figure()\n",
    "x = np.array(epochs)\n",
    "y = np.array(xfer_loss)\n",
    "x_smooth = np.linspace(x.min(), x.max(), 500)\n",
    "y_smooth = make_interp_spline(x,y)(x_smooth)\n",
    "plt.plot(x_smooth, y_smooth, 'r-', label = 'Entrenamiento')\n",
    "\n",
    "x1 = np.array(epochs)\n",
    "y1 = np.array(val_loss)\n",
    "x1_smooth = np.linspace(x1.min(), x1.max(), 500)\n",
    "y1_smooth = make_interp_spline(x1,y1)(x1_smooth)\n",
    "\n",
    "plt.plot(x1_smooth, y1_smooth, 'g-', label = 'Validación')\n",
    "plt.title('Transfer Learning - Pérdida de entrenamiento y validación')\n",
    "plt.legend(loc = 'upper right', fontsize = 9)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.ylim(0,max(y1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_files = glob.glob(\"./data/test/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = cv2.imread(predict_files[0])\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "im = cv2.resize(im, (256, 256)).astype(np.float32)\n",
    "im = np.expand_dims(im, axis = 0)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor, image_id = [], []\n",
    "for i in predict_files:\n",
    "    im = cv2.imread(i)\n",
    "    im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (256, 256)).astype(np.float32) / 255.0\n",
    "    im = np.expand_dims(im, axis =0)\n",
    "    outcome = [np.argmax(model.predict(im))]\n",
    "    predictor.extend(list(outcome))\n",
    "    image_id.extend([i.rsplit(\"\\\\\")[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = pd.DataFrame()\n",
    "final[\"id\"] = image_id\n",
    "final[\"Noodles\"] = predictor\n",
    "final.head(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = train_generator.class_indices\n",
    "classes = {value : key for key, value in classes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final[\"Noodles\"] = final[\"Noodles\"].apply(lambda x: classes[x])\n",
    "final.head(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grafcar** para poder ver todo el set de la predicción de las 2 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "longitud, altura = 150, 150\n",
    "clases = ['noodles_fried','noodle_soup']\n",
    "path =  \"./data/test\"\n",
    "Ima = os.listdir(path)\n",
    "for w in Ima:\n",
    "    RutaImg=path+'/'+w\n",
    "    img = load_img(RutaImg, target_size=(longitud, altura))\n",
    "    x = img_to_array(img)\n",
    "    x = x / 255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    array = model.predict(x)\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    predicted_label = np.argmax(array[0])\n",
    "    if (w.find(clases[0]) != -1):\n",
    "        i = 0\n",
    "    elif (w.find(clases[1]) != -1):\n",
    "        i = 1 \n",
    "\n",
    "    if (predicted_label == i):\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    \n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(clases[predicted_label],100*np.max(array[0]),clases[i]),color=color)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(2))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(2), array[0],color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(array[0])\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[i].set_color('blue')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
